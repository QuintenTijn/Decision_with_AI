{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8bc82e2",
   "metadata": {},
   "source": [
    "# Imports & loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e71aeb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from support import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "from tqdm.notebook import tqdm \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import optuna\n",
    "from optuna.visualization import (\n",
    "    plot_optimization_history,\n",
    "    plot_param_importances,\n",
    "    plot_parallel_coordinate,\n",
    "    plot_slice,\n",
    "    plot_contour,\n",
    ")\n",
    "import plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d542257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch, gc, os\n",
    "\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available():         # Apple GPU\n",
    "        return torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():               # NVIDIA eGPU etc.\n",
    "        return torch.device(\"cuda\")\n",
    "    else:                                         # fallback\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "torch.set_float32_matmul_precision(\"high\")        # speeds M-series matmul\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"   # CPU fallback for missing ops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2cd22c",
   "metadata": {},
   "source": [
    "#### Print Python and PyTorch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2b3f703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 15:03:56) [MSC v.1929 64 bit (AMD64)]\n",
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Python version:\", os.sys.version)\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e93b79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pin_memory_flag   = device.type == 'cuda'\n",
    "# num_workers_flag  = 2 if device.type == 'cuda' else 0   # 0 on M-series/CPU\n",
    "# print(f\"Pin memory: {pin_memory_flag}, Num workers: {num_workers_flag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c934f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch, time\n",
    "# x = torch.randn(8192, 8192, device='mps')\n",
    "# t0 = time.perf_counter()\n",
    "# y = x @ x.T\n",
    "# torch.mps.synchronize()     # wait for GPU\n",
    "# print(\"took %.2f s\" % (time.perf_counter() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e549c95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset()\n",
    "train_dataset = data[0]\n",
    "test_dataset = data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1fe5181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([3, 60, 30])\n",
      "Label: 0\n",
      "Train_dataset: Dataset ImageFolder\n",
      "    Number of datapoints: 136\n",
      "    Root location: WF-data/train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=[105, 78], interpolation=bilinear, max_size=None, antialias=True)\n",
      "               CenterCrop(size=[60, 30])\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "img, label = train_dataset[0]\n",
    "\n",
    "print(f\"Image shape: {img.shape}\")\n",
    "print(f\"Label: {label}\")\n",
    "print(f\"Train_dataset: {train_dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d010dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_tensor, label = train_dataset[0]\n",
    "transform_to_pil = T.ToPILImage()\n",
    "img_pil = transform_to_pil(img_tensor)\n",
    "\n",
    "plt.imshow(img_pil)\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be77b74f",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e629039",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a7eeaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, input_channels, input_height, input_width, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 16 x H/2 x W/2\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)   # Output: 32 x H/4 x W/4\n",
    "        )\n",
    "        \n",
    "        # Calculate the flattened size dynamically to connect conv layers to fc layers\n",
    "        with torch.no_grad(): # We don't need gradients for this calculation\n",
    "            dummy_input = torch.zeros(1, input_channels, input_height, input_width)\n",
    "            flattened_size = self.conv_layers(dummy_input).flatten(1).shape[1]\n",
    "            \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(flattened_size, 128),\n",
    "            nn.ReLU(),\n",
    "            # Output 1 logit for binary classification (labels 0 or 1)\n",
    "            # This will be used with BCEWithLogitsLoss\n",
    "            nn.Linear(128, 1 if num_classes == 2 else num_classes) \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.flatten(1) # Flatten all dimensions except batch\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc4d073b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected image shape: Channels=3, Height=60, Width=30\n",
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2531bfa8aeb4451a8ed46eee9b3dc8ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/40 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "BackendCompilerFailed",
     "evalue": "backend='inductor' raised:\nRuntimeError: Found Quadro P1000 which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.1\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py:1446\u001b[0m, in \u001b[0;36mOutputGraph._call_user_compiler\u001b[1;34m(self, gm)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     compiler_fn \u001b[38;5;241m=\u001b[39m WrapperBackend(compiler_fn)\n\u001b[1;32m-> 1446\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m compiler_fn(gm, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexample_inputs())\n\u001b[0;32m   1447\u001b[0m _step_logger()(logging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone compiler function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py:129\u001b[0m, in \u001b[0;36mWrapBackendDebug.__call__\u001b[1;34m(self, gm, example_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 129\u001b[0m     compiled_gm \u001b[38;5;241m=\u001b[39m compiler_fn(gm, example_inputs)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\__init__.py:2234\u001b[0m, in \u001b[0;36m_TorchCompileInductorWrapper.__call__\u001b[1;34m(self, model_, inputs_)\u001b[0m\n\u001b[0;32m   2232\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile_fx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compile_fx\n\u001b[1;32m-> 2234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compile_fx(model_, inputs_, config_patches\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:1521\u001b[0m, in \u001b[0;36mcompile_fx\u001b[1;34m(model_, example_inputs_, inner_compile, config_patches, decompositions)\u001b[0m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m V\u001b[38;5;241m.\u001b[39mset_fake_mode(fake_mode), torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mtracing(\n\u001b[0;32m   1517\u001b[0m     tracing_context\n\u001b[0;32m   1518\u001b[0m ), compiled_autograd\u001b[38;5;241m.\u001b[39mdisable(), functorch_config\u001b[38;5;241m.\u001b[39mpatch(\n\u001b[0;32m   1519\u001b[0m     unlift_effect_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1520\u001b[0m ):\n\u001b[1;32m-> 1521\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m aot_autograd(\n\u001b[0;32m   1522\u001b[0m         fw_compiler\u001b[38;5;241m=\u001b[39mfw_compiler,\n\u001b[0;32m   1523\u001b[0m         bw_compiler\u001b[38;5;241m=\u001b[39mbw_compiler,\n\u001b[0;32m   1524\u001b[0m         inference_compiler\u001b[38;5;241m=\u001b[39minference_compiler,\n\u001b[0;32m   1525\u001b[0m         decompositions\u001b[38;5;241m=\u001b[39mdecompositions,\n\u001b[0;32m   1526\u001b[0m         partition_fn\u001b[38;5;241m=\u001b[39mpartition_fn,\n\u001b[0;32m   1527\u001b[0m         keep_inference_input_mutations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1528\u001b[0m         cudagraphs\u001b[38;5;241m=\u001b[39mcudagraphs,\n\u001b[0;32m   1529\u001b[0m     )(model_, example_inputs_)\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\backends\\common.py:72\u001b[0m, in \u001b[0;36mAotAutograd.__call__\u001b[1;34m(self, gm, example_inputs, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m enable_aot_logging(), patch_config:\n\u001b[1;32m---> 72\u001b[0m     cg \u001b[38;5;241m=\u001b[39m aot_module_simplified(gm, example_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     73\u001b[0m     counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot_autograd\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py:1071\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[1;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, keep_inference_input_mutations, inference_compiler, cudagraphs)\u001b[0m\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1071\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m dispatch_and_compile()\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mGmWrapper):\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;66;03m# This function is called by the flatten_graph_inputs wrapper, which boxes\u001b[39;00m\n\u001b[0;32m   1075\u001b[0m     \u001b[38;5;66;03m# the inputs so that they can be freed before the end of this scope.\u001b[39;00m\n\u001b[0;32m   1076\u001b[0m     \u001b[38;5;66;03m# For overhead reasons, this is not the default wrapper, see comment:\u001b[39;00m\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/122535/files#r1560096481\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py:1056\u001b[0m, in \u001b[0;36maot_module_simplified.<locals>.dispatch_and_compile\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compiled_autograd\u001b[38;5;241m.\u001b[39mdisable():\n\u001b[1;32m-> 1056\u001b[0m     compiled_fn, _ \u001b[38;5;241m=\u001b[39m create_aot_dispatcher_function(\n\u001b[0;32m   1057\u001b[0m         functional_call,\n\u001b[0;32m   1058\u001b[0m         fake_flat_args,\n\u001b[0;32m   1059\u001b[0m         aot_config,\n\u001b[0;32m   1060\u001b[0m         fake_mode,\n\u001b[0;32m   1061\u001b[0m         shape_env,\n\u001b[0;32m   1062\u001b[0m     )\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py:522\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[1;34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_aot_dispatcher_function\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 522\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _create_aot_dispatcher_function(\n\u001b[0;32m    523\u001b[0m         flat_fn, fake_flat_args, aot_config, fake_mode, shape_env\n\u001b[0;32m    524\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py:759\u001b[0m, in \u001b[0;36m_create_aot_dispatcher_function\u001b[1;34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[0m\n\u001b[0;32m    757\u001b[0m compiler_fn \u001b[38;5;241m=\u001b[39m choose_dispatcher(needs_autograd, aot_config)\n\u001b[1;32m--> 759\u001b[0m compiled_fn, fw_metadata \u001b[38;5;241m=\u001b[39m compiler_fn(\n\u001b[0;32m    760\u001b[0m     flat_fn,\n\u001b[0;32m    761\u001b[0m     _dup_fake_script_obj(fake_flat_args),\n\u001b[0;32m    762\u001b[0m     aot_config,\n\u001b[0;32m    763\u001b[0m     fw_metadata\u001b[38;5;241m=\u001b[39mfw_metadata,\n\u001b[0;32m    764\u001b[0m )\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn, fw_metadata\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py:588\u001b[0m, in \u001b[0;36maot_dispatch_autograd\u001b[1;34m(flat_fn, flat_args, aot_config, fw_metadata)\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TracingContext\u001b[38;5;241m.\u001b[39mreport_output_strides() \u001b[38;5;28;01mas\u001b[39;00m fwd_output_strides:\n\u001b[1;32m--> 588\u001b[0m     compiled_fw_func \u001b[38;5;241m=\u001b[39m aot_config\u001b[38;5;241m.\u001b[39mfw_compiler(fw_module, adjusted_flat_args)\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(compiled_fw_func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:1350\u001b[0m, in \u001b[0;36mcompile_fx.<locals>.fw_compiler_base\u001b[1;34m(model, example_inputs, is_inference)\u001b[0m\n\u001b[0;32m   1349\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_utils\u001b[38;5;241m.\u001b[39mdynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompile_fx.<locals>.fw_compiler_base\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _fw_compiler_base(model, example_inputs, is_inference)\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:1421\u001b[0m, in \u001b[0;36mcompile_fx.<locals>._fw_compiler_base\u001b[1;34m(model, example_inputs, is_inference)\u001b[0m\n\u001b[0;32m   1413\u001b[0m     user_visible_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mfromkeys(\n\u001b[0;32m   1414\u001b[0m         n\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m   1415\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m model_outputs[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1418\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(n, torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mNode)\n\u001b[0;32m   1419\u001b[0m     )\n\u001b[1;32m-> 1421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inner_compile(\n\u001b[0;32m   1422\u001b[0m     model,\n\u001b[0;32m   1423\u001b[0m     example_inputs,\n\u001b[0;32m   1424\u001b[0m     static_input_idxs\u001b[38;5;241m=\u001b[39mget_static_input_idxs(fixed),\n\u001b[0;32m   1425\u001b[0m     cudagraphs\u001b[38;5;241m=\u001b[39mcudagraphs,\n\u001b[0;32m   1426\u001b[0m     graph_id\u001b[38;5;241m=\u001b[39mgraph_id,\n\u001b[0;32m   1427\u001b[0m     is_inference\u001b[38;5;241m=\u001b[39mis_inference,\n\u001b[0;32m   1428\u001b[0m     boxed_forward_device_index\u001b[38;5;241m=\u001b[39mforward_device,\n\u001b[0;32m   1429\u001b[0m     user_visible_outputs\u001b[38;5;241m=\u001b[39muser_visible_outputs,\n\u001b[0;32m   1430\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:475\u001b[0m, in \u001b[0;36mcompile_fx_inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    473\u001b[0m stack\u001b[38;5;241m.\u001b[39menter_context(DebugContext())\n\u001b[1;32m--> 475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_compiler_debug(_compile_fx_inner, compiler_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minductor\u001b[39m\u001b[38;5;124m\"\u001b[39m)(\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    477\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_aot.py:85\u001b[0m, in \u001b[0;36mwrap_compiler_debug.<locals>.debug_wrapper\u001b[1;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;66;03m# Call the compiler_fn - which is either aot_autograd or inductor\u001b[39;00m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# with fake inputs\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     inner_compiled_fn \u001b[38;5;241m=\u001b[39m compiler_fn(gm, example_inputs)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# TODO: Failures here are troublesome because no real inputs,\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;66;03m# need a different serialization strategy\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:661\u001b[0m, in \u001b[0;36m_compile_fx_inner\u001b[1;34m(gm, example_inputs, cudagraphs, static_input_idxs, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, boxed_forward_device_index, user_visible_outputs, layout_opt, extern_node_serializer)\u001b[0m\n\u001b[0;32m    659\u001b[0m             \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39m_is_inductor_static \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 661\u001b[0m     compiled_graph \u001b[38;5;241m=\u001b[39m FxGraphCache\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m    662\u001b[0m         codegen_and_compile,\n\u001b[0;32m    663\u001b[0m         gm,\n\u001b[0;32m    664\u001b[0m         example_inputs,\n\u001b[0;32m    665\u001b[0m         graph_kwargs,\n\u001b[0;32m    666\u001b[0m         inputs_to_check,\n\u001b[0;32m    667\u001b[0m         local\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mfx_graph_cache,\n\u001b[0;32m    668\u001b[0m         remote\u001b[38;5;241m=\u001b[39mfx_graph_remote_cache,\n\u001b[0;32m    669\u001b[0m     )\n\u001b[0;32m    670\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_inductor\\codecache.py:1334\u001b[0m, in \u001b[0;36mFxGraphCache.load\u001b[1;34m(compile_fx_fn, gm, example_inputs, fx_kwargs, inputs_to_check, local, remote)\u001b[0m\n\u001b[0;32m   1333\u001b[0m cache_event_time \u001b[38;5;241m=\u001b[39m start_time\n\u001b[1;32m-> 1334\u001b[0m compiled_graph \u001b[38;5;241m=\u001b[39m compile_fx_fn(\n\u001b[0;32m   1335\u001b[0m     gm, example_inputs, inputs_to_check, fx_kwargs\n\u001b[0;32m   1336\u001b[0m )\n\u001b[0;32m   1337\u001b[0m compiled_graph\u001b[38;5;241m.\u001b[39m_time_taken_ns \u001b[38;5;241m=\u001b[39m time_ns() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:570\u001b[0m, in \u001b[0;36m_compile_fx_inner.<locals>.codegen_and_compile\u001b[1;34m(gm, example_inputs, inputs_to_check, fx_kwargs)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;124;03mThis function calls fx_codegen_and_compile and also adds some extra metadata to the resulting\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;124;03mcompiled fx graph. The metadata is saved to FXGraphCache.\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m compiled_graph \u001b[38;5;241m=\u001b[39m fx_codegen_and_compile(gm, example_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfx_kwargs)\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(compiled_graph, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;66;03m# We only return a string in aot mode, in which case we don't\u001b[39;00m\n\u001b[0;32m    573\u001b[0m     \u001b[38;5;66;03m# need to do any post-compilation steps: we just return the string,\u001b[39;00m\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;66;03m# which is the filename of the compiled code.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:878\u001b[0m, in \u001b[0;36mfx_codegen_and_compile\u001b[1;34m(gm, example_inputs, cudagraphs, static_input_idxs, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, user_visible_outputs, layout_opt, extern_node_serializer)\u001b[0m\n\u001b[0;32m    877\u001b[0m _check_triton_bf16_support(graph)\n\u001b[1;32m--> 878\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mcompile_to_fn()\n\u001b[0;32m    879\u001b[0m num_bytes, nodes_num_elem, node_runtimes \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mcount_bytes()\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_inductor\\graph.py:1913\u001b[0m, in \u001b[0;36mGraphLowering.compile_to_fn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1912\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1913\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompile_to_module()\u001b[38;5;241m.\u001b[39mcall\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_inductor\\graph.py:1839\u001b[0m, in \u001b[0;36mGraphLowering.compile_to_module\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1836\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[0;32m   1837\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraphLowering.compile_to_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_gen\u001b[39m\u001b[38;5;124m\"\u001b[39m, fwd_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1838\u001b[0m ):\n\u001b[1;32m-> 1839\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compile_to_module()\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_inductor\\graph.py:1845\u001b[0m, in \u001b[0;36mGraphLowering._compile_to_module\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1842\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcodecache\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyCodeCache\n\u001b[0;32m   1844\u001b[0m code, linemap \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1845\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcodegen_with_cpp_wrapper() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpp_wrapper \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcodegen()\n\u001b[0;32m   1846\u001b[0m )\n\u001b[0;32m   1848\u001b[0m GraphLowering\u001b[38;5;241m.\u001b[39msave_output_code(code)\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_inductor\\graph.py:1780\u001b[0m, in \u001b[0;36mGraphLowering.codegen\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1778\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_wrapper_code()\n\u001b[1;32m-> 1780\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler \u001b[38;5;241m=\u001b[39m Scheduler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperations)\n\u001b[0;32m   1781\u001b[0m V\u001b[38;5;241m.\u001b[39mdebug\u001b[38;5;241m.\u001b[39mdraw_orig_fx_graph(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_gm, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mnodes)\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_inductor\\scheduler.py:1731\u001b[0m, in \u001b[0;36mScheduler.__init__\u001b[1;34m(self, nodes)\u001b[0m\n\u001b[0;32m   1730\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScheduler.__init__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init(nodes)\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_inductor\\scheduler.py:1749\u001b[0m, in \u001b[0;36mScheduler._init\u001b[1;34m(self, nodes)\u001b[0m\n\u001b[0;32m   1741\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavailable_buffer_names \u001b[38;5;241m=\u001b[39m OrderedSet(\n\u001b[0;32m   1742\u001b[0m     [\n\u001b[0;32m   1743\u001b[0m         \u001b[38;5;241m*\u001b[39mV\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mgraph_inputs\u001b[38;5;241m.\u001b[39mkeys(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1746\u001b[0m     ]\n\u001b[0;32m   1747\u001b[0m )\n\u001b[1;32m-> 1749\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_scheduler_node(n) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes]\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_zero_dim_cpu_tensor()\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_inductor\\scheduler.py:1856\u001b[0m, in \u001b[0;36mScheduler.create_scheduler_node\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, (ir\u001b[38;5;241m.\u001b[39mComputedBuffer, ir\u001b[38;5;241m.\u001b[39mTemplateBuffer)):\n\u001b[1;32m-> 1856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SchedulerNode(\u001b[38;5;28mself\u001b[39m, node)\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, ir\u001b[38;5;241m.\u001b[39mExternKernel):\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_inductor\\scheduler.py:833\u001b[0m, in \u001b[0;36mSchedulerNode.__init__\u001b[1;34m(self, scheduler, node)\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_from_node(node)\n\u001b[1;32m--> 833\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_attrs()\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_inductor\\scheduler.py:846\u001b[0m, in \u001b[0;36mSchedulerNode._compute_attrs\u001b[1;34m(self, extra_indexing_constraints, recompute_sizes_body_func)\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sizes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39msimplify_and_reorder(\n\u001b[0;32m    842\u001b[0m     extra_indexing_constraints\u001b[38;5;241m=\u001b[39mextra_indexing_constraints,\n\u001b[0;32m    843\u001b[0m     recompute_sizes_body_func\u001b[38;5;241m=\u001b[39mrecompute_sizes_body_func,\n\u001b[0;32m    844\u001b[0m )\n\u001b[1;32m--> 846\u001b[0m group_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mget_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mget_device())\u001b[38;5;241m.\u001b[39mgroup_fn\n\u001b[0;32m    847\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mget_device(), group_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sizes))\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_inductor\\scheduler.py:3360\u001b[0m, in \u001b[0;36mScheduler.get_backend\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m   3359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackends:\n\u001b[1;32m-> 3360\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackends[device] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_backend(device)\n\u001b[0;32m   3361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackends[device]\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_inductor\\scheduler.py:3348\u001b[0m, in \u001b[0;36mScheduler.create_backend\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m   3344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3345\u001b[0m     device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3346\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (device_props \u001b[38;5;241m:=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_device_properties(device))\u001b[38;5;241m.\u001b[39mmajor \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m7\u001b[39m\n\u001b[0;32m   3347\u001b[0m ):\n\u001b[1;32m-> 3348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   3349\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_props\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_props\u001b[38;5;241m.\u001b[39mmajor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_props\u001b[38;5;241m.\u001b[39mminor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa: B950\u001b[39;00m\n\u001b[0;32m   3350\u001b[0m     )\n\u001b[0;32m   3351\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_gpu(device\u001b[38;5;241m.\u001b[39mtype):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Found Quadro P1000 which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mBackendCompilerFailed\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m     39\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:465\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    460\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    461\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[0;32m    462\u001b[0m )\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[0;32m    469\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[0;32m    470\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1269\u001b[0m, in \u001b[0;36mCatchErrorsWrapper.__call__\u001b[1;34m(self, frame, cache_entry, frame_state)\u001b[0m\n\u001b[0;32m   1263\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(\n\u001b[0;32m   1264\u001b[0m                 frame, cache_entry, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks, frame_state\n\u001b[0;32m   1265\u001b[0m             )\n\u001b[0;32m   1267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[0;32m   1268\u001b[0m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[1;32m-> 1269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_torchdynamo_orig_callable(\n\u001b[0;32m   1270\u001b[0m         frame, cache_entry, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks, frame_state, skip\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1271\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1064\u001b[0m, in \u001b[0;36mConvertFrame.__call__\u001b[1;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[0;32m   1062\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1064\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_convert(\n\u001b[0;32m   1065\u001b[0m         frame, cache_entry, hooks, frame_state, skip\u001b[38;5;241m=\u001b[39mskip \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1066\u001b[0m     )\n\u001b[0;32m   1067\u001b[0m     counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1068\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:526\u001b[0m, in \u001b[0;36mConvertFrameAssert.__call__\u001b[1;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[0;32m    510\u001b[0m compile_id \u001b[38;5;241m=\u001b[39m CompileId(frame_id, frame_compile_id)\n\u001b[0;32m    512\u001b[0m signpost_event(\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_convert_frame_assert._compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m     },\n\u001b[0;32m    524\u001b[0m )\n\u001b[1;32m--> 526\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _compile(\n\u001b[0;32m    527\u001b[0m     frame\u001b[38;5;241m.\u001b[39mf_code,\n\u001b[0;32m    528\u001b[0m     frame\u001b[38;5;241m.\u001b[39mf_globals,\n\u001b[0;32m    529\u001b[0m     frame\u001b[38;5;241m.\u001b[39mf_locals,\n\u001b[0;32m    530\u001b[0m     frame\u001b[38;5;241m.\u001b[39mf_builtins,\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_torchdynamo_orig_callable,\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_one_graph,\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_export,\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_export_constraints,\n\u001b[0;32m    535\u001b[0m     hooks,\n\u001b[0;32m    536\u001b[0m     cache_entry,\n\u001b[0;32m    537\u001b[0m     cache_size,\n\u001b[0;32m    538\u001b[0m     frame,\n\u001b[0;32m    539\u001b[0m     frame_state\u001b[38;5;241m=\u001b[39mframe_state,\n\u001b[0;32m    540\u001b[0m     compile_id\u001b[38;5;241m=\u001b[39mcompile_id,\n\u001b[0;32m    541\u001b[0m     skip\u001b[38;5;241m=\u001b[39mskip \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    542\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:924\u001b[0m, in \u001b[0;36m_compile\u001b[1;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[0;32m    922\u001b[0m guarded_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 924\u001b[0m     guarded_code \u001b[38;5;241m=\u001b[39m compile_inner(code, one_graph, hooks, transform)\n\u001b[0;32m    925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m guarded_code\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:666\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[1;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_compile.compile_inner\u001b[39m\u001b[38;5;124m\"\u001b[39m, phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentire_frame_compile\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    665\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m CompileTimeInstructionCounter\u001b[38;5;241m.\u001b[39mrecord():\n\u001b[1;32m--> 666\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _compile_inner(code, one_graph, hooks, transform)\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_utils_internal.py:87\u001b[0m, in \u001b[0;36mcompile_time_strobelight_meta.<locals>.compile_time_strobelight_meta_inner.<locals>.wrapper_function\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39mprofile_compile_time(\n\u001b[0;32m     90\u001b[0m     function, phase_name, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m     91\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:699\u001b[0m, in \u001b[0;36m_compile.<locals>._compile_inner\u001b[1;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[0;32m    697\u001b[0m CompileContext\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39mattempt \u001b[38;5;241m=\u001b[39m attempt\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 699\u001b[0m     out_code \u001b[38;5;241m=\u001b[39m transform_code_object(code, transform)\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mRestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py:1322\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[1;34m(code, transformations, safe)\u001b[0m\n\u001b[0;32m   1319\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[0;32m   1320\u001b[0m propagate_line_nums(instructions)\n\u001b[1;32m-> 1322\u001b[0m transformations(instructions, code_options)\n\u001b[0;32m   1323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:219\u001b[0m, in \u001b[0;36mpreserve_global_state.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m exit_stack\u001b[38;5;241m.\u001b[39menter_context(\n\u001b[0;32m    216\u001b[0m     torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39m_symbolic_trace\u001b[38;5;241m.\u001b[39m_maybe_revert_all_patches()\n\u001b[0;32m    217\u001b[0m )\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    221\u001b[0m     cleanup\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:634\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[1;34m(instructions, code_options)\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtracing_context), tracer\u001b[38;5;241m.\u001b[39mset_current_tx():\n\u001b[1;32m--> 634\u001b[0m         tracer\u001b[38;5;241m.\u001b[39mrun()\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mUnspecializeRestartAnalysis:\n\u001b[0;32m    636\u001b[0m     speculation_log\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:2796\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 2796\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:983\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    982\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 983\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep():\n\u001b[0;32m    984\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:895\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 895\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_table[inst\u001b[38;5;241m.\u001b[39mopcode](\u001b[38;5;28mself\u001b[39m, inst)\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[0;32m    897\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:2987\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[1;34m(self, inst)\u001b[0m\n\u001b[0;32m   2986\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mRETURN_VALUE\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst):\n\u001b[1;32m-> 2987\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(inst)\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:2972\u001b[0m, in \u001b[0;36mInstructionTranslator._return\u001b[1;34m(self, inst)\u001b[0m\n\u001b[0;32m   2967\u001b[0m _step_logger()(\n\u001b[0;32m   2968\u001b[0m     logging\u001b[38;5;241m.\u001b[39mINFO,\n\u001b[0;32m   2969\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchdynamo done tracing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minst\u001b[38;5;241m.\u001b[39mopname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2970\u001b[0m )\n\u001b[0;32m   2971\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m triggered compile\u001b[39m\u001b[38;5;124m\"\u001b[39m, inst\u001b[38;5;241m.\u001b[39mopname)\n\u001b[1;32m-> 2972\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mcompile_subgraph(\n\u001b[0;32m   2973\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2974\u001b[0m     reason\u001b[38;5;241m=\u001b[39mGraphCompileReason(\n\u001b[0;32m   2975\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_value\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe_summary()], graph_break\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   2976\u001b[0m     ),\n\u001b[0;32m   2977\u001b[0m )\n\u001b[0;32m   2978\u001b[0m return_inst \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2979\u001b[0m     create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2980\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inst\u001b[38;5;241m.\u001b[39mopname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2981\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_CONST\u001b[39m\u001b[38;5;124m\"\u001b[39m, argval\u001b[38;5;241m=\u001b[39minst\u001b[38;5;241m.\u001b[39margval)\n\u001b[0;32m   2982\u001b[0m )\n\u001b[0;32m   2983\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39madd_output_instructions([return_inst])\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py:1117\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[1;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[0;32m   1114\u001b[0m append_prefix_insts()\n\u001b[0;32m   1115\u001b[0m \u001b[38;5;66;03m# optimization to generate better code in a common case\u001b[39;00m\n\u001b[0;32m   1116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_output_instructions(\n\u001b[1;32m-> 1117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompile_and_call_fx_graph(tx, \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mreversed\u001b[39m(stack_values)), root)\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;241m+\u001b[39m [create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNPACK_SEQUENCE\u001b[39m\u001b[38;5;124m\"\u001b[39m, arg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(stack_values))]\n\u001b[0;32m   1119\u001b[0m )\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;66;03m# restore all the live local vars\u001b[39;00m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_output_instructions(\n\u001b[0;32m   1122\u001b[0m     [PyCodegen(tx)\u001b[38;5;241m.\u001b[39mcreate_store(var) \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(restore_vars)]\n\u001b[0;32m   1123\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py:1369\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[1;34m(self, tx, rv, root)\u001b[0m\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracing_context\u001b[38;5;241m.\u001b[39mfake_mode \u001b[38;5;241m=\u001b[39m backend_fake_mode\n\u001b[0;32m   1368\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_global_state():\n\u001b[1;32m-> 1369\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_user_compiler(gm)\n\u001b[0;32m   1371\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lazy_graph_module\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _LazyGraphModule\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(compiled_fn, _LazyGraphModule) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1374\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(compiled_fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), _LazyGraphModule)\n\u001b[0;32m   1375\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m compiled_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_lazy_forward\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;66;03m# this is a _LazyGraphModule. This makes it easier for dynamo to\u001b[39;00m\n\u001b[0;32m   1380\u001b[0m     \u001b[38;5;66;03m# optimize a _LazyGraphModule.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py:1416\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[1;34m(self, gm)\u001b[0m\n\u001b[0;32m   1412\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_user_compiler\u001b[39m(\u001b[38;5;28mself\u001b[39m, gm: fx\u001b[38;5;241m.\u001b[39mGraphModule) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CompiledFn:\n\u001b[0;32m   1413\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[0;32m   1414\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputGraph.call_user_compiler\u001b[39m\u001b[38;5;124m\"\u001b[39m, phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackend_compile\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1415\u001b[0m     ):\n\u001b[1;32m-> 1416\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_user_compiler(gm)\n",
      "File \u001b[1;32mc:\\Users\\20193217\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py:1465\u001b[0m, in \u001b[0;36mOutputGraph._call_user_compiler\u001b[1;34m(self, gm)\u001b[0m\n\u001b[0;32m   1463\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1465\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BackendCompilerFailed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiler_fn, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1467\u001b[0m signpost_event(\n\u001b[0;32m   1468\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1469\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputGraph.call_user_compiler\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1475\u001b[0m     },\n\u001b[0;32m   1476\u001b[0m )\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "\u001b[1;31mBackendCompilerFailed\u001b[0m: backend='inductor' raised:\nRuntimeError: Found Quadro P1000 which is too old to be supported by the triton GPU compiler, which is used as the backend. Triton only supports devices of CUDA Capability >= 7.0, but your device is of CUDA capability 6.1\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "sample_img, _ = train_dataset[0]\n",
    "C, H, W = sample_img.shape\n",
    "print(f\"Detected image shape: Channels={C}, Height={H}, Width={W}\")\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.007\n",
    "batch_size = 16 # Adjust this based on memory\n",
    "num_epochs = 40  # Adjust as needed for convergence\n",
    "num_classes = 2  # Binary classification (labels 0 and 1)\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# DataLoaders\n",
    "pin_memory_flag   = device.type == 'cuda'\n",
    "num_workers_flag  = 2 if device.type == 'cuda' else 0   # 0 on M-series/CPU\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers_flag, pin_memory=pin_memory_flag)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers_flag, pin_memory=pin_memory_flag)\n",
    "\n",
    "model = SimpleCNN(input_channels=C, input_height=H, input_width=W, num_classes=num_classes).to(device)\n",
    "model = torch.compile(model)\n",
    "\n",
    "# Binary Cross Entropy with Logits Loss, suitable for binary classification with one output neuron\n",
    "criterion = nn.BCEWithLogitsLoss() # Different choiches possible\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Training Progress\", unit=\"epoch\"):\n",
    "    model.train()  \n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        # Reshape labels to [batch_size, 1] and convert to float for BCEWithLogitsLoss\n",
    "        labels = labels.to(device).float().unsqueeze(1) \n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        # if (i + 1) % max(1, len(train_loader) // 10) == 0: # Print progress, e.g. 10 times per epoch\n",
    "            # print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_epoch_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] completed. Average Training Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "    # Evaluation on the test set\n",
    "    model.eval()  \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad(): \n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device).float().unsqueeze(1) #\n",
    "            \n",
    "            # predictions on the test set\n",
    "            outputs = model(images)\n",
    "            predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Accuracy on test set after epoch {epoch+1}: {accuracy:.2f}%\")\n",
    "\n",
    "    if device.type == \"mps\":            # Apple-GPU path\n",
    "        torch.mps.empty_cache()\n",
    "    elif device.type == \"cuda\":         # NVIDIA-GPU path\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect() \n",
    "\n",
    "    \n",
    "# Save the model\n",
    "model_path = \"simple_cnn_model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(\"Finished Training\")\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbfdaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img, _ = train_dataset[0]\n",
    "C, H, W = sample_img.shape\n",
    "print(f\"Detected image shape: Channels={C}, Height={H}, Width={W}\")\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.007\n",
    "batch_size = 16 # Adjust this based on memory\n",
    "num_epochs = 40  # Adjust as needed for convergence\n",
    "num_classes = 2  # Binary classification (labels 0 and 1)\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# DataLoaders\n",
    "pin_memory_flag   = device.type == 'cuda'\n",
    "num_workers_flag  = 2 if device.type == 'cuda' else 0   # 0 on M-series/CPU\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers_flag, pin_memory=pin_memory_flag)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers_flag, pin_memory=pin_memory_flag)\n",
    "\n",
    "model = SimpleCNN(input_channels=C, input_height=H, input_width=W, num_classes=num_classes).to(device)\n",
    "model = torch.compile(model)\n",
    "\n",
    "# Binary Cross Entropy with Logits Loss, suitable for binary classification with one output neuron\n",
    "criterion = nn.BCEWithLogitsLoss() # Different choiches possible\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Training Progress\", unit=\"epoch\"):\n",
    "    model.train()  \n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        # Reshape labels to [batch_size, 1] and convert to float for BCEWithLogitsLoss\n",
    "        labels = labels.to(device).float().unsqueeze(1) \n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        # if (i + 1) % max(1, len(train_loader) // 10) == 0: # Print progress, e.g. 10 times per epoch\n",
    "            # print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_epoch_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] completed. Average Training Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "    # Evaluation on the test set\n",
    "    model.eval()  \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad(): \n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device).float().unsqueeze(1) #\n",
    "            \n",
    "            # predictions on the test set\n",
    "            outputs = model(images)\n",
    "            predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Accuracy on test set after epoch {epoch+1}: {accuracy:.2f}%\")\n",
    "\n",
    "    if device.type == \"mps\":            # Apple-GPU path\n",
    "        torch.mps.empty_cache()\n",
    "    elif device.type == \"cuda\":         # NVIDIA-GPU path\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect() \n",
    "\n",
    "    \n",
    "# Save the model\n",
    "model_path = \"simple_cnn_model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(\"Finished Training\")\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c0f870",
   "metadata": {},
   "source": [
    "## Increased complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d470596",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, input_channels, input_height, input_width, num_classes,\n",
    "                 conv1_out_channels=16, conv2_out_channels=32, fc_neurons=128):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, conv1_out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: conv1_out_channels x H/2 x W/2\n",
    "            nn.Conv2d(conv1_out_channels, conv2_out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)   # Output: conv2_out_channels x H/4 x W/4\n",
    "        )\n",
    "        \n",
    "        # Calculate the flattened size dynamically\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, input_channels, input_height, input_width)\n",
    "            flattened_size = self.conv_layers(dummy_input).flatten(1).shape[1]\n",
    "            \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(flattened_size, fc_neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc_neurons, 1 if num_classes == 2 else num_classes) \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.flatten(1) # Flatten all dimensions except batch\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e13a536",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img, _ = train_dataset[0]\n",
    "C, H, W = sample_img.shape\n",
    "print(f\"Detected image shape: Channels={C}, Height={H}, Width={W}\")\n",
    "\n",
    "# Hyperparameters (Base Model)\n",
    "learning_rate = 0.001\n",
    "batch_size = 16 \n",
    "num_epochs = 40 \n",
    "num_classes = 2 \n",
    "\n",
    "# Default architectural parameters for the base model\n",
    "conv1_out_channels_base = 16\n",
    "conv2_out_channels_base = 32\n",
    "fc_neurons_base = 128\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# DataLoaders\n",
    "pin_memory_flag   = device.type == 'cuda'\n",
    "num_workers_flag  = 2 if device.type == 'cuda' else 0   # 0 on M-series/CPU\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers_flag, pin_memory=pin_memory_flag)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers_flag, pin_memory=pin_memory_flag)\n",
    "\n",
    "# Use the (potentially modified) SimpleCNN class\n",
    "model = SimpleCNN(input_channels=C, input_height=H, input_width=W, num_classes=num_classes,\n",
    "                    conv1_out_channels=conv1_out_channels_base, \n",
    "                    conv2_out_channels=conv2_out_channels_base, \n",
    "                    fc_neurons=fc_neurons_base).to(device)\n",
    "model = torch.compile(model)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Lists to store metrics for plotting\n",
    "epoch_train_losses = []\n",
    "epoch_test_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Training Progress\", unit=\"epoch\"):\n",
    "    model.train()  \n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device).float().unsqueeze(1) \n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0) # loss.item() is avg loss for batch\n",
    "    \n",
    "    avg_epoch_loss = running_loss / len(train_loader.dataset)  # Average loss over the entire training set\n",
    "    epoch_train_losses.append(avg_epoch_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] completed. Average Training Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "    # Evaluation on the test set\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad(): \n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device).float().unsqueeze(1)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float() \n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    epoch_test_accuracies.append(accuracy)\n",
    "    print(f\"Accuracy on test set after epoch {epoch+1}: {accuracy:.2f}%\")\n",
    "\n",
    "    if device.type == \"mps\":            # Apple-GPU path\n",
    "        torch.mps.empty_cache()\n",
    "    elif device.type == \"cuda\":         # NVIDIA-GPU path\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()                        # reclaim Python-held objects\n",
    "\n",
    "print(\"Finished Training Base Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6293f4b5",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2ffc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting learning curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), epoch_train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve', fontsize=14)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), epoch_test_accuracies, label='Test Accuracy', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Test Accuracy Curve', fontsize=14)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca0754c",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e629c83",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59f0e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = get_device()\n",
    "print(f\"Using device for hyperparameter tuning: {device}\")\n",
    "\n",
    "# Get image dimensions (C, H, W should be available from the previous cell or re-derived)\n",
    "if 'C' not in locals() or 'H' not in locals() or 'W' not in locals():\n",
    "    sample_img, _ = train_dataset[0]\n",
    "    C, H, W = sample_img.shape\n",
    "    print(f\"Re-detected image shape: Channels={C}, Height={H}, Width={W}\")\n",
    "\n",
    "# Import KFold\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np # Ensure numpy is imported\n",
    "import matplotlib.pyplot as plt # Ensure matplotlib is imported for trial plots\n",
    "\n",
    "# Hyperparameter search space\n",
    "param_space = {\n",
    "    'learning_rate': [0.0001, 0.0005, 0.001, 0.005, 0.01],\n",
    "    'batch_size': [8, 16, 32],\n",
    "    'conv1_out_channels': [8, 16, 32],\n",
    "    'conv2_out_channels': [16, 32, 64],\n",
    "    'fc_neurons': [64, 128, 256],\n",
    "}\n",
    "\n",
    "# Store history for the best model's learning curves\n",
    "best_model_train_losses = []\n",
    "best_model_val_accuracies = []\n",
    "\n",
    "num_classes_hp = 2\n",
    "num_epochs_search = 20 # Epochs for each fold in CV and for final retraining\n",
    "num_trials = 20 # Number of random hyperparameter sets to try\n",
    "n_splits_cv = 5 # Number of folds for cross-validation\n",
    "\n",
    "best_hyperparams = None\n",
    "best_avg_cv_accuracy = -1.0\n",
    "# These will store learning curves for the FINAL retrained model\n",
    "final_retrained_model_train_losses = []\n",
    "final_retrained_model_test_accuracies = [] # Using test set for the final model's curve\n",
    "\n",
    "perfect_found = False # If average CV accuracy hits 100%\n",
    "\n",
    "for trial in tqdm(range(num_trials), desc=\"Hyperparameter Search Trials\", unit=\"trial\", leave=True):\n",
    "    if perfect_found:\n",
    "        print(\"Perfect hyperparameters found based on CV, stopping further trials.\")\n",
    "        break\n",
    "\n",
    "    current_params = {k: random.choice(v) for k, v in param_space.items()}\n",
    "    print(f\"\\nTrial {trial+1}/{num_trials} - Testing params: {current_params}\")\n",
    "\n",
    "    kf = KFold(n_splits=n_splits_cv, shuffle=True, random_state=trial) # Use trial for different shuffles or a fixed one\n",
    "    \n",
    "    trial_fold_final_accuracies = []\n",
    "    # Store epoch-wise validation accuracies for each fold, to average later for plotting\n",
    "    # Each element in the outer list corresponds to an epoch\n",
    "    # Each inner list will contain validation accuracies from all folds for that epoch\n",
    "    trial_all_folds_epoch_val_accuracies = [[] for _ in range(num_epochs_search)]\n",
    "\n",
    "    pin_memory_flag = device.type == 'cuda'\n",
    "    num_workers_flag = 2 if device.type == 'cuda' else 0\n",
    "\n",
    "    for fold_idx, (train_ids, val_ids) in enumerate(kf.split(train_dataset)):\n",
    "        print(f\"  Fold {fold_idx+1}/{n_splits_cv}\")\n",
    "        \n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "        val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
    "\n",
    "        try:\n",
    "            # Use train_dataset for both training and validation subsets in CV\n",
    "            current_train_loader = DataLoader(train_dataset, batch_size=current_params['batch_size'], sampler=train_subsampler, num_workers=num_workers_flag, pin_memory=pin_memory_flag)\n",
    "            current_val_loader = DataLoader(train_dataset, batch_size=current_params['batch_size'], sampler=val_subsampler, num_workers=num_workers_flag, pin_memory=pin_memory_flag)\n",
    "        except Exception as e:\n",
    "            print(f\"  Error creating DataLoader for fold {fold_idx+1}, possibly due to batch size: {e}. Skipping fold.\")\n",
    "            # Add a placeholder or handle this case for averaging if needed, e.g., append 0 accuracy\n",
    "            trial_fold_final_accuracies.append(0) \n",
    "            for epoch_list in trial_all_folds_epoch_val_accuracies:\n",
    "                 # to maintain structure if a fold fails early, or handle more gracefully\n",
    "                if len(epoch_list) == fold_idx : epoch_list.append(0)\n",
    "            continue\n",
    "            \n",
    "        model_hp = SimpleCNN(input_channels=C, input_height=H, input_width=W, num_classes=num_classes_hp,\n",
    "                                conv1_out_channels=current_params['conv1_out_channels'],\n",
    "                                conv2_out_channels=current_params['conv2_out_channels'],\n",
    "                                fc_neurons=current_params['fc_neurons']).to(device)\n",
    "        \n",
    "        criterion_hp = nn.BCEWithLogitsLoss()\n",
    "        optimizer_hp = optim.Adam(model_hp.parameters(), lr=current_params['learning_rate'])\n",
    "\n",
    "        fold_epoch_val_accuracies = [] # Stores val accuracies for current fold's epochs\n",
    "\n",
    "        for epoch in tqdm(range(num_epochs_search), desc=f\"    Epochs (Fold {fold_idx+1})\", leave=False):\n",
    "            model_hp.train()\n",
    "            epoch_running_loss = 0.0\n",
    "            for images, labels in current_train_loader:\n",
    "                images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n",
    "                optimizer_hp.zero_grad()\n",
    "                outputs = model_hp(images)\n",
    "                loss = criterion_hp(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer_hp.step()\n",
    "                epoch_running_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            # avg_epoch_train_loss = epoch_running_loss / len(train_subsampler) # For fold-specific train loss\n",
    "\n",
    "            model_hp.eval()\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            with torch.no_grad():\n",
    "                for images, labels in current_val_loader:\n",
    "                    images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n",
    "                    outputs = model_hp(images)\n",
    "                    predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            current_epoch_val_accuracy = 100 * val_correct / val_total if val_total > 0 else 0.0\n",
    "            fold_epoch_val_accuracies.append(current_epoch_val_accuracy)\n",
    "            trial_all_folds_epoch_val_accuracies[epoch].append(current_epoch_val_accuracy)\n",
    "\n",
    "            if device.type == \"mps\": torch.mps.empty_cache()\n",
    "            elif device.type == \"cuda\": torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "        trial_fold_final_accuracies.append(fold_epoch_val_accuracies[-1] if fold_epoch_val_accuracies else 0.0)\n",
    "        del model_hp, optimizer_hp, criterion_hp, current_train_loader, current_val_loader # Cleanup\n",
    "        if device.type == \"mps\": torch.mps.empty_cache()\n",
    "        elif device.type == \"cuda\": torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    current_trial_avg_final_cv_accuracy = np.mean(trial_fold_final_accuracies) if trial_fold_final_accuracies else 0.0\n",
    "    print(f\"  Trial {trial+1} Avg CV Accuracy: {current_trial_avg_final_cv_accuracy:.2f}%\")\n",
    "\n",
    "    # Calculate average epoch-wise validation accuracy for the current trial\n",
    "    current_trial_avg_epoch_cv_accuracies = [np.mean(epoch_accs) if epoch_accs else 0.0 for epoch_accs in trial_all_folds_epoch_val_accuracies]\n",
    "\n",
    "    # Plot validation accuracy for the current trial\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(1, num_epochs_search + 1), current_trial_avg_epoch_cv_accuracies, marker='o', linestyle='-')\n",
    "    plt.title(f\"Trial {trial+1} - Avg CV Validation Accuracy\\nParams: {current_params}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Average CV Validation Accuracy (%)\")\n",
    "    plt.ylim(0, 101) # Ensure y-axis covers 0-100%\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    if current_trial_avg_final_cv_accuracy > best_avg_cv_accuracy:\n",
    "        best_avg_cv_accuracy = current_trial_avg_final_cv_accuracy\n",
    "        best_hyperparams = current_params\n",
    "        print(f\"  New best average CV accuracy: {best_avg_cv_accuracy:.2f}% with params: {best_hyperparams}\")\n",
    "\n",
    "    if best_avg_cv_accuracy == 100.0:\n",
    "        perfect_found = True\n",
    "        # No need to break inner loops, already done with this trial. Outer loop will break.\n",
    "\n",
    "print(\"\\nFinished Hyperparameter Search.\")\n",
    "if best_hyperparams:\n",
    "    print(f\"Best hyperparameters found: {best_hyperparams}\")\n",
    "    print(f\"Best average CV validation accuracy: {best_avg_cv_accuracy:.2f}%\")\n",
    "\n",
    "    # Retrain the model on the full training dataset with the best hyperparameters\n",
    "    print(\"\\nRetraining model with best hyperparameters on the full training dataset...\")\n",
    "    \n",
    "    # DataLoaders for final training and testing\n",
    "    final_train_loader = DataLoader(train_dataset, batch_size=best_hyperparams['batch_size'], shuffle=True, num_workers=num_workers_flag, pin_memory=pin_memory_flag)\n",
    "    # The test_loader will be used for evaluation during/after retraining\n",
    "    final_test_loader = DataLoader(test_dataset, batch_size=best_hyperparams['batch_size'], shuffle=False, num_workers=num_workers_flag, pin_memory=pin_memory_flag)\n",
    "\n",
    "    final_model = SimpleCNN(input_channels=C, input_height=H, input_width=W, num_classes=num_classes_hp,\n",
    "                            conv1_out_channels=best_hyperparams['conv1_out_channels'],\n",
    "                            conv2_out_channels=best_hyperparams['conv2_out_channels'],\n",
    "                            fc_neurons=best_hyperparams['fc_neurons']).to(device)\n",
    "    \n",
    "    criterion_final = nn.BCEWithLogitsLoss()\n",
    "    optimizer_final = optim.Adam(final_model.parameters(), lr=best_hyperparams['learning_rate'])\n",
    "\n",
    "    # Use global lists for the final model's learning curves\n",
    "    # These will be used by the plotting cell (deffcba1) if it expects these variable names\n",
    "    best_model_train_losses.clear() # Clearing in case they were used before\n",
    "    best_model_val_accuracies.clear() # Renaming to avoid confusion with CV val accuracies\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs_search), desc=\"Final Model Retraining\", unit=\"epoch\"): # Using num_epochs_search for retraining\n",
    "        final_model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in final_train_loader:\n",
    "            images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n",
    "            optimizer_final.zero_grad()\n",
    "            outputs = final_model(images)\n",
    "            loss = criterion_final(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer_final.step()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        avg_epoch_loss = running_loss / len(final_train_loader.dataset)\n",
    "        best_model_train_losses.append(avg_epoch_loss) # Storing for final plot\n",
    "\n",
    "        # Evaluate on the test set during retraining\n",
    "        final_model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in final_test_loader: # Using test_dataset for this evaluation\n",
    "                images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n",
    "                outputs = final_model(images)\n",
    "                predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        accuracy = 100 * correct / total if total > 0 else 0.0\n",
    "        best_model_val_accuracies.append(accuracy) # Storing for final plot (using test accuracy)\n",
    "        print(f\"  Epoch {epoch+1}/{num_epochs_search} - Train Loss: {avg_epoch_loss:.4f}, Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "        if device.type == \"mps\": torch.mps.empty_cache()\n",
    "        elif device.type == \"cuda\": torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # Save the retrained best model\n",
    "    best_randomsearch_model_path = \"best_randomsearch_cnn_model.pth\"\n",
    "    torch.save(final_model.state_dict(), best_randomsearch_model_path)\n",
    "    print(f\"Best retrained model saved to {best_randomsearch_model_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"No best hyperparameters found (e.g., all trials failed or num_trials was 0).\")\n",
    "    # Ensure these lists are empty if no model is trained, for the plotting cell\n",
    "    best_model_train_losses = []\n",
    "    best_model_val_accuracies = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622afaba",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f6e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = list(range(1, len(best_model_train_losses) + 1))\n",
    "\n",
    "print(f\"Best Validation Accuracy: {best_val_accuracy:.2f}%\")\n",
    "print(f\"Best Hyperparameters: {best_hyperparams}\")\n",
    "print(f\"Number of epochs needed: {len(epochs)}\")\n",
    "\n",
    "\n",
    "# Plotting learning curves for the best model from hyperparameter search\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, best_model_train_losses, label='Best Model Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Best Model - Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, best_model_val_accuracies, label='Best Model Validation Accuracy', color='red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Best Model - Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Load the best model state for further use if needed:\n",
    "# model_best = SimpleCNN(input_channels=C, input_height=H, input_width=W, num_classes=num_classes_hp, **best_hyperparams_arch).to(device)\n",
    "# model_best.load_state_dict(best_model_state)\n",
    "# where best_hyperparams_arch = {k: v for k,v in best_hyperparams.items() if k in ['conv1_out_channels', 'conv2_out_channels', 'fc_neurons']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4038ab6b",
   "metadata": {},
   "source": [
    "### Method 2 - Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f1a1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install optuna\n",
    "# %pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e515a56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = get_device()\n",
    "print(f\"Using device for Optuna hyperparameter tuning: {device}\")\n",
    "\n",
    "# Get image dimensions\n",
    "sample_img, _ = train_dataset[0]\n",
    "C, H, W = sample_img.shape\n",
    "print(f\"Image shape: Channels={C}, Height={H}, Width={W}\")\n",
    "\n",
    "num_classes_hp = 2\n",
    "num_epochs_search = 40 # Fewer epochs for faster search, adjust as needed for Optuna trials\n",
    "num_trials_optuna = 10 # Define number of trials\n",
    "\n",
    "def stop_when_perfect(study, trial):\n",
    "    if trial.value is not None and trial.value >= 100.0:\n",
    "        study.stop()\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameter suggestions\n",
    "    lr = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
    "    conv1_out = trial.suggest_categorical(\"conv1_out_channels\", [8, 16, 32, 64])\n",
    "    conv2_out = trial.suggest_categorical(\"conv2_out_channels\", [16, 32, 64, 128])\n",
    "    fc_neurons_val = trial.suggest_categorical(\"fc_neurons\", [64, 128, 256])\n",
    "\n",
    "    # DataLoaders\n",
    "    pin_memory_flag   = device.type == 'cuda'\n",
    "    num_workers_flag  = 2 if device.type == 'cuda' else 0   # 0 on M-series/CPU\n",
    "\n",
    "    try:\n",
    "        current_train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers_flag, pin_memory=pin_memory_flag)\n",
    "        current_val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers_flag, pin_memory=pin_memory_flag)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating DataLoader for trial {trial.number}: {e}. Pruning trial.\")\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "\n",
    "    # Model (Ensure SimpleCNN class is defined and accessible)\n",
    "    model_opt = SimpleCNN(input_channels=C, input_height=H, input_width=W, num_classes=num_classes_hp,\n",
    "                          conv1_out_channels=conv1_out,\n",
    "                          conv2_out_channels=conv2_out,\n",
    "                          fc_neurons=fc_neurons_val).to(device)\n",
    "\n",
    "    criterion_opt = nn.BCEWithLogitsLoss()\n",
    "    optimizer_opt = optim.Adam(model_opt.parameters(), lr=lr)\n",
    "\n",
    "    trial_train_losses = []\n",
    "    trial_val_accuracies = []\n",
    "\n",
    "    patience = 10\n",
    "    best_val_acc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(num_epochs_search), desc=f\"Trial {trial.number} [Training]\", leave=True):\n",
    "        model_opt.train()\n",
    "        epoch_running_loss = 0.0\n",
    "        for images, labels in current_train_loader: # train_pbar_opt:\n",
    "            images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n",
    "            \n",
    "            optimizer_opt.zero_grad(set_to_none=True)\n",
    "            loss = criterion_opt(model_opt(images), labels)\n",
    "            loss.backward()\n",
    "            optimizer_opt.step()\n",
    "\n",
    "            epoch_running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        trial_train_losses.append(epoch_running_loss / len(train_loader.dataset))\n",
    "\n",
    "        # Validation\n",
    "        model_opt.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in current_val_loader: # val_pbar_opt:\n",
    "                images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n",
    "                outputs = model_opt(images)\n",
    "                predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        current_epoch_val_accuracy = 100 * val_correct / val_total if val_total > 0 else 0\n",
    "        trial_val_accuracies.append(current_epoch_val_accuracy)\n",
    "        \n",
    "        if current_epoch_val_accuracy > best_val_acc:\n",
    "            best_val_acc = current_epoch_val_accuracy\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1} for trial {trial.number}. No improvement in validation accuracy for {patience} epochs.\")\n",
    "            break\n",
    "\n",
    "        if current_epoch_val_accuracy == 100.0:\n",
    "            print(f\"Perfect validation accuracy reached at epoch {epoch+1} for trial {trial.number}. Stopping early.\")\n",
    "            break\n",
    "\n",
    "        # Optuna pruning (optional, but good for long searches)\n",
    "        trial.report(current_epoch_val_accuracy, epoch)\n",
    "        if trial.should_prune():\n",
    "            # Store partial curves if pruned\n",
    "            trial.set_user_attr(\"train_losses\", trial_train_losses)\n",
    "            trial.set_user_attr(\"val_accuracies\", trial_val_accuracies)\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    # Store full learning curves in user attributes for the trial\n",
    "    trial.set_user_attr(\"train_losses\", trial_train_losses)\n",
    "    trial.set_user_attr(\"val_accuracies\", trial_val_accuracies)\n",
    "\n",
    "    del model_opt, optimizer_opt, criterion_opt, current_train_loader, current_val_loader  \n",
    "    if device.type == \"mps\":                                   \n",
    "        torch.mps.empty_cache()\n",
    "    elif device.type == \"cuda\":                                \n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()   \n",
    "\n",
    "    return best_val_acc\n",
    "    # return trial_val_accuracies[-1] # Return final validation accuracy\n",
    "\n",
    "# Create a study object and optimize\n",
    "# You can add a pruner, e.g., optuna.pruners.MedianPruner()\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_min_trials=3, n_warmup_steps=3))\n",
    "# study.optimize(objective, n_trials=num_trials_optuna, gc_after_trial=True)\n",
    "study.optimize(objective, n_trials=num_trials_optuna, gc_after_trial=True, callbacks=[stop_when_perfect], show_progress_bar=True)\n",
    "\n",
    "\n",
    "print(\"\\nFinished Optuna Hyperparameter Search.\")\n",
    "best_trial_optuna = study.best_trial\n",
    "best_hyperparams_optuna = best_trial_optuna.params\n",
    "best_val_accuracy_optuna = best_trial_optuna.value\n",
    "\n",
    "\n",
    "# Retrieve learning curves for the best trial\n",
    "best_model_train_losses_optuna = best_trial_optuna.user_attrs.get(\"train_losses\", [])\n",
    "best_model_val_accuracies_optuna = best_trial_optuna.user_attrs.get(\"val_accuracies\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c719e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plot_optimization_history(study)\n",
    "fig1.show()  # in Jupyter this will render an interactive Plotly chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7100d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plot_param_importances(study)\n",
    "fig2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f6d509",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3 = plot_parallel_coordinate(study)\n",
    "fig3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb9b7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig4 = plot_slice(study, params=[\"learning_rate\", \"batch_size\", \"conv1_out_channels\"])\n",
    "fig4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bead9ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig5 = plot_contour(study, params=[\"learning_rate\", \"batch_size\"])\n",
    "fig5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8134c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = best_trial_optuna.user_attrs.get(\"train_losses\", [])\n",
    "val_accs     = best_trial_optuna.user_attrs.get(\"val_accuracies\", [])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"epoch\": list(range(1, len(train_losses) + 1)),\n",
    "    \"train_loss\": train_losses,\n",
    "    \"val_accuracy\": val_accs,\n",
    "})\n",
    "df.to_csv(\"best_trial_curves.csv\", index=False)\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1553d2e",
   "metadata": {},
   "source": [
    "## Retrain best optuna model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68268c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # After Optuna optimization, retrain the best model with the best hyperparameters and save it\n",
    "\n",
    "# # Extract best hyperparameters from the Optuna study\n",
    "# best_params = study.best_trial.params\n",
    "\n",
    "# # Dataloaders with the best batch size\n",
    "# pin_memory_flag   = device.type == 'cuda'\n",
    "# num_workers_flag  = 2 if device.type == 'cuda' else 0   # 0 on M-series/CPU\n",
    "\n",
    "# train_loader_opt = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True, num_workers=num_workers_flag, pin_memory=pin_memory_flag)\n",
    "# val_loader_opt = DataLoader(test_dataset, batch_size=best_params['batch_size'], shuffle=False, num_workers=num_workers_flag, pin_memory=pin_memory_flag)\n",
    "\n",
    "# # Re-create and train the model with the best hyperparameters\n",
    "# model_optimal = SimpleCNN(\n",
    "#     input_channels=C, input_height=H, input_width=W, num_classes=num_classes_hp,\n",
    "#     conv1_out_channels=best_params['conv1_out_channels'],\n",
    "#     conv2_out_channels=best_params['conv2_out_channels'],\n",
    "#     fc_neurons=best_params['fc_neurons']\n",
    "# ).to(device)\n",
    "\n",
    "# criterion_opt = nn.BCEWithLogitsLoss()\n",
    "# optimizer_opt = optim.Adam(model_optimal.parameters(), lr=best_params['learning_rate'])\n",
    "# num_epochs_opt = num_epochs_search  # Or set to a higher value for final training\n",
    "\n",
    "# for epoch in tqdm(range(num_epochs_opt), desc=f\"Epoch {trial+1} Training\", leave=True):\n",
    "# # for epoch in range(num_epochs_opt):\n",
    "#     model_optimal.train()\n",
    "#     for images, labels in train_loader_opt:\n",
    "#         images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n",
    "#         optimizer_opt.zero_grad()\n",
    "#         outputs = model_optimal(images)\n",
    "#         loss = criterion_opt(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer_opt.step()\n",
    "\n",
    "# # Save the trained optimal model\n",
    "# best_optuna_model_path = \"best_optuna_cnn_model.pth\"\n",
    "# torch.save(model_optimal.state_dict(), best_optuna_model_path)\n",
    "# print(f\"Optimal model saved to {best_optuna_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b63142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After Optuna optimization, retrain the best model with the best hyperparameters and save it\n",
    "\n",
    "# Extract best hyperparameters from the Optuna study\n",
    "best_params = study.best_trial.params\n",
    "\n",
    "# Dataloaders with the best batch size\n",
    "pin_memory_flag   = device.type == 'cuda'\n",
    "num_workers_flag  = 2 if device.type == 'cuda' else 0   # 0 on M-series/CPU\n",
    "\n",
    "train_loader_opt = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True, num_workers=num_workers_flag, pin_memory=pin_memory_flag)\n",
    "val_loader_opt = DataLoader(test_dataset, batch_size=best_params['batch_size'], shuffle=False, num_workers=num_workers_flag, pin_memory=pin_memory_flag)\n",
    "\n",
    "# Re-create and train the model with the best hyperparameters\n",
    "model_optimal = SimpleCNN(\n",
    "    input_channels=C, input_height=H, input_width=W, num_classes=num_classes_hp,\n",
    "    conv1_out_channels=best_params['conv1_out_channels'],\n",
    "    conv2_out_channels=best_params['conv2_out_channels'],\n",
    "    fc_neurons=best_params['fc_neurons']\n",
    ").to(device)\n",
    "\n",
    "criterion_opt = nn.BCEWithLogitsLoss()\n",
    "optimizer_opt = optim.Adam(model_optimal.parameters(), lr=best_params['learning_rate'])\n",
    "num_epochs_opt = num_epochs_search  # Or set to a higher value for final training\n",
    "\n",
    "best_model_train_losses_optuna = []\n",
    "best_model_val_accuracies_optuna = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs_opt), desc=f\"Epoch {trial+1} Training\", leave=True):\n",
    "# for epoch in range(num_epochs_opt):\n",
    "    model_optimal.train()\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for images, labels in train_loader_opt:\n",
    "        images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n",
    "        optimizer_opt.zero_grad()\n",
    "        outputs = model_optimal(images)\n",
    "        loss = criterion_opt(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_opt.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_train_loss = running_loss / num_batches\n",
    "    best_model_train_losses_optuna.append(avg_train_loss)\n",
    "\n",
    "    # --- 5a. Evaluate on validation set ---\n",
    "    model_optimal.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images_val, labels_val in val_loader_opt:\n",
    "            images_val = images_val.to(device)\n",
    "            labels_val = labels_val.to(device).float().unsqueeze(1)\n",
    "\n",
    "            logits = model_optimal(images_val)\n",
    "            preds = torch.sigmoid(logits) >= 0.5\n",
    "            correct += (preds.int() == labels_val.int()).sum().item()\n",
    "            total += labels_val.size(0)\n",
    "\n",
    "    val_accuracy = 100.0 * correct / total\n",
    "    best_model_val_accuracies_optuna.append(val_accuracy)\n",
    "    \n",
    "best_val_accuracy_optuna = max(best_model_val_accuracies_optuna)\n",
    "# Save the trained optimal model\n",
    "best_optuna_model_path = \"best_optuna_cnn_model.pth\"\n",
    "torch.save(model_optimal.state_dict(), best_optuna_model_path)\n",
    "print(f\"Optimal model saved to {best_optuna_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1912c0ea",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6caca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best Validation Accuracy (from Optuna study): {best_val_accuracy_optuna:.0f}%\")\n",
    "print(f\"Best Hyperparameters (from Optuna study): {best_hyperparams_optuna}\")\n",
    "print(f\"Number of epochs needed: {num_epochs_opt}\")\n",
    "\n",
    "# Plotting learning curves for the best model from Optuna hyperparameter search\n",
    "if best_model_train_losses_optuna and best_model_val_accuracies_optuna:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, len(best_model_train_losses_optuna) + 1), best_model_train_losses_optuna, label='Best Model Training Loss (Optuna)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Best Model (Optuna) - Training Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, len(best_model_val_accuracies_optuna) + 1), best_model_val_accuracies_optuna, label='Best Model Validation Accuracy (Optuna)', color='red')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Best Model (Optuna) - Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Learning curve data for the best Optuna trial is not available.\")\n",
    "    \n",
    "# You can now load the best model state for further use if needed.\n",
    "# First, you'd typically retrain a model with best_hyperparams_optuna for a full number of epochs\n",
    "# and save its state_dict. The state_dict is not directly saved by the Optuna objective function above.\n",
    "# Example:\n",
    "# model_best_optuna = SimpleCNN(input_channels=C, input_height=H, input_width=W, num_classes=num_classes_hp,\n",
    "#                               conv1_out_channels=best_hyperparams_optuna['conv1_out_channels'],\n",
    "#                               conv2_out_channels=best_hyperparams_optuna['conv2_out_channels'],\n",
    "#                               fc_neurons=best_hyperparams_optuna['fc_neurons']).to(device)\n",
    "# # Then, you would train this model_best_optuna using the best learning rate, optimizer, and batch size.\n",
    "# # For now, we are just plotting the curves from the optimization search itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0f5298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Retrieve best hyperparameters from the Optuna study ---\n",
    "best_hyperparams_optuna = study.best_trial.params\n",
    "\n",
    "# --- 2. Create dataloaders using the best batch size ---\n",
    "pin_memory_flag  = (device.type == 'cuda')\n",
    "num_workers_flag = 2 if device.type == 'cuda' else 0\n",
    "\n",
    "train_loader_opt = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=best_hyperparams_optuna['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers_flag,\n",
    "    pin_memory=pin_memory_flag\n",
    ")\n",
    "\n",
    "val_loader_opt = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=best_hyperparams_optuna['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers_flag,\n",
    "    pin_memory=pin_memory_flag\n",
    ")\n",
    "\n",
    "# --- 3. Re‐create the model with the best hyperparameters ---\n",
    "model_optimal = SimpleCNN(\n",
    "    input_channels=C,\n",
    "    input_height=H,\n",
    "    input_width=W,\n",
    "    num_classes=num_classes_hp,\n",
    "    conv1_out_channels=best_hyperparams_optuna['conv1_out_channels'],\n",
    "    conv2_out_channels=best_hyperparams_optuna['conv2_out_channels'],\n",
    "    fc_neurons=best_hyperparams_optuna['fc_neurons']\n",
    ").to(device)\n",
    "\n",
    "criterion_opt = nn.BCEWithLogitsLoss()\n",
    "optimizer_opt = optim.Adam(\n",
    "    model_optimal.parameters(),\n",
    "    lr=best_hyperparams_optuna['learning_rate']\n",
    ")\n",
    "\n",
    "num_epochs_opt = num_epochs_search  # or increase if you want longer final training\n",
    "\n",
    "# --- 4. Containers to record per‐epoch training loss and validation accuracy ---\n",
    "best_model_train_losses_optuna = []\n",
    "best_model_val_accuracies_optuna = []\n",
    "\n",
    "# --- 5. Final training loop (over num_epochs_opt) ---\n",
    "for epoch in tqdm(range(num_epochs_opt), desc=\"Final Training\", leave=True):\n",
    "    model_optimal.train()\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for images, labels in train_loader_opt:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device).float().unsqueeze(1)\n",
    "\n",
    "        optimizer_opt.zero_grad()\n",
    "        outputs = model_optimal(images)\n",
    "        loss = criterion_opt(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_opt.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    # Compute average training loss for this epoch\n",
    "    avg_train_loss = running_loss / num_batches\n",
    "    best_model_train_losses_optuna.append(avg_train_loss)\n",
    "\n",
    "    # --- 5a. Evaluate on validation set ---\n",
    "    model_optimal.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images_val, labels_val in val_loader_opt:\n",
    "            images_val = images_val.to(device)\n",
    "            labels_val = labels_val.to(device).float().unsqueeze(1)\n",
    "\n",
    "            logits = model_optimal(images_val)\n",
    "            preds = torch.sigmoid(logits) >= 0.5\n",
    "            correct += (preds.int() == labels_val.int()).sum().item()\n",
    "            total += labels_val.size(0)\n",
    "\n",
    "    val_accuracy = 100.0 * correct / total\n",
    "    best_model_val_accuracies_optuna.append(val_accuracy)\n",
    "\n",
    "# --- 6. Determine the best validation accuracy achieved during final training ---\n",
    "best_val_accuracy_optuna = max(best_model_val_accuracies_optuna)\n",
    "\n",
    "# --- 7. Save the trained optimal model ---\n",
    "best_optuna_model_path = \"best_optuna_cnn_model.pth\"\n",
    "torch.save(model_optimal.state_dict(), best_optuna_model_path)\n",
    "\n",
    "# --- 8. Print out the results ---\n",
    "print(f\"Best Validation Accuracy (final training): {best_val_accuracy_optuna:.2f}%\")\n",
    "print(f\"Best Hyperparameters (from Optuna study): {best_hyperparams_optuna}\")\n",
    "print(f\"Optimal model saved to {best_optuna_model_path}\")\n",
    "\n",
    "# --- 9. Plot learning curves for the final model ---\n",
    "if best_model_train_losses_optuna and best_model_val_accuracies_optuna:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Training Loss Curve\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(\n",
    "        range(1, len(best_model_train_losses_optuna) + 1),\n",
    "        best_model_train_losses_optuna,\n",
    "        label='Training Loss'\n",
    "    )\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Final Model Training Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Validation Accuracy Curve\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(\n",
    "        range(1, len(best_model_val_accuracies_optuna) + 1),\n",
    "        best_model_val_accuracies_optuna,\n",
    "        label='Validation Accuracy',\n",
    "        color='red'\n",
    "    )\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Final Model Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Learning curve data for the final model is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb0ff7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
